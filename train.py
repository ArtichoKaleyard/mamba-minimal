"""Vision Mamba MNISTè®­ç»ƒè„šæœ¬

å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    - æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
    - æ¨¡å‹è®­ç»ƒä¸éªŒè¯
    - å­¦ä¹ ç‡è°ƒåº¦
    - æ¨¡å‹ä¿å­˜ä¸åŠ è½½
    - è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
    - æ—¥å¿—è®°å½•

ä½¿ç”¨æ–¹æ³•:
    python train.py --epochs 5 --batch_size 256 --lr 0.001
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torch.utils.tensorboard import SummaryWriter

import argparse
import os
import time
from tqdm import tqdm
from typing import Tuple, Dict
import json
from pathlib import Path

from vision_mamba import create_vision_mamba_mnist, VisionMamba


class Trainer:
    """è®­ç»ƒå™¨ç±»ï¼šå°è£…å®Œæ•´çš„è®­ç»ƒæµç¨‹"""

    def __init__(
            self,
            model: VisionMamba,
            train_loader: DataLoader,
            val_loader: DataLoader,
            criterion: nn.Module,
            optimizer: optim.Optimizer,
            scheduler: optim.lr_scheduler._LRScheduler,
            device: torch.device,
            save_dir: str = "./checkpoints",
            log_dir: str = "./logs"
    ):
        """åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            model: Vision Mambaæ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            criterion: æŸå¤±å‡½æ•°
            optimizer: ä¼˜åŒ–å™¨
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
            device: è®­ç»ƒè®¾å¤‡ (cuda/cpu)
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            log_dir: æ—¥å¿—ä¿å­˜ç›®å½•
        """
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device

        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–TensorBoard
        self.writer = SummaryWriter(log_dir)

        # è®­ç»ƒç»Ÿè®¡
        self.best_val_acc = 0.0
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []

    def train_epoch(self, epoch: int) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch

        Args:
            epoch: å½“å‰epochç¼–å·

        Returns:
            avg_loss: å¹³å‡è®­ç»ƒæŸå¤±
            avg_acc: å¹³å‡è®­ç»ƒå‡†ç¡®ç‡
        """
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch} [Train]")

        for batch_idx, (images, labels) in enumerate(pbar):
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            images = images.to(self.device)
            labels = labels.to(self.device)

            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(images)
            loss = self.criterion(outputs, labels)

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            self.optimizer.step()

            # ç»Ÿè®¡
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100. * correct / total:.2f}%'
            })

            # è®°å½•åˆ°TensorBoardï¼ˆæ¯100ä¸ªbatchè®°å½•ä¸€æ¬¡ï¼‰
            if batch_idx % 100 == 0:
                global_step = epoch * len(self.train_loader) + batch_idx
                self.writer.add_scalar('Train/BatchLoss', loss.item(), global_step)

        avg_loss = running_loss / len(self.train_loader)
        avg_acc = 100. * correct / total

        return avg_loss, avg_acc

    @torch.no_grad()
    def validate(self, epoch: int) -> Tuple[float, float]:
        """éªŒè¯æ¨¡å‹æ€§èƒ½

        Args:
            epoch: å½“å‰epochç¼–å·

        Returns:
            avg_loss: å¹³å‡éªŒè¯æŸå¤±
            avg_acc: å¹³å‡éªŒè¯å‡†ç¡®ç‡
        """
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0

        pbar = tqdm(self.val_loader, desc=f"Epoch {epoch} [Val]")

        for images, labels in pbar:
            images = images.to(self.device)
            labels = labels.to(self.device)

            # å‰å‘ä¼ æ’­
            outputs = self.model(images)
            loss = self.criterion(outputs, labels)

            # ç»Ÿè®¡
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{100. * correct / total:.2f}%'
            })

        avg_loss = running_loss / len(self.val_loader)
        avg_acc = 100. * correct / total

        return avg_loss, avg_acc

    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹

        Args:
            epoch: å½“å‰epochç¼–å·
            is_best: æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_acc': self.best_val_acc,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accs': self.train_accs,
            'val_accs': self.val_accs
        }

        # ä¿å­˜æœ€æ–°æ¨¡å‹
        latest_path = self.save_dir / 'latest_checkpoint.pth'
        torch.save(checkpoint, latest_path)

        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜ä¸€ä»½
        if is_best:
            best_path = self.save_dir / 'best_model.pth'
            torch.save(checkpoint, best_path)
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")

    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹

        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.best_val_acc = checkpoint['best_val_acc']
        self.train_losses = checkpoint['train_losses']
        self.val_losses = checkpoint['val_losses']
        self.train_accs = checkpoint['train_accs']
        self.val_accs = checkpoint['val_accs']

        return checkpoint['epoch']

    def train(self, num_epochs: int, resume: str = None):
        """å®Œæ•´è®­ç»ƒæµç¨‹

        Args:
            num_epochs: è®­ç»ƒè½®æ•°
            resume: æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        start_epoch = 0

        # å¦‚æœæŒ‡å®šäº†æ¢å¤è®­ç»ƒ
        if resume:
            start_epoch = self.load_checkpoint(resume) + 1
            print(f"ä»epoch {start_epoch}ç»§ç»­è®­ç»ƒ")

        print(f"\n{'=' * 60}")
        print(f"å¼€å§‹è®­ç»ƒ Vision Mamba on MNIST")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ€»epochs: {num_epochs}")
        print(f"è®­ç»ƒé›†å¤§å°: {len(self.train_loader.dataset)}")
        print(f"éªŒè¯é›†å¤§å°: {len(self.val_loader.dataset)}")
        print(f"{'=' * 60}\n")

        for epoch in range(start_epoch, num_epochs):
            epoch_start_time = time.time()

            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            self.train_losses.append(train_loss)
            self.train_accs.append(train_acc)

            # éªŒè¯
            val_loss, val_acc = self.validate(epoch)
            self.val_losses.append(val_loss)
            self.val_accs.append(val_acc)

            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']

            # è®¡ç®—epochç”¨æ—¶
            epoch_time = time.time() - epoch_start_time

            # è®°å½•åˆ°TensorBoard
            self.writer.add_scalar('Train/Loss', train_loss, epoch)
            self.writer.add_scalar('Train/Accuracy', train_acc, epoch)
            self.writer.add_scalar('Val/Loss', val_loss, epoch)
            self.writer.add_scalar('Val/Accuracy', val_acc, epoch)
            self.writer.add_scalar('Learning_Rate', current_lr, epoch)

            # æ‰“å°epochæ€»ç»“
            print(f"\nEpoch {epoch} æ€»ç»“:")
            print(f"  è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
            print(f"  éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")
            print(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
            print(f"  ç”¨æ—¶: {epoch_time:.2f}s")

            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best = val_acc > self.best_val_acc
            if is_best:
                self.best_val_acc = val_acc
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(epoch, is_best)
            print(f"{'-' * 60}\n")

        print(f"\n{'=' * 60}")
        print(f"è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.2f}%")
        print(f"{'=' * 60}\n")

        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()

        self.writer.close()

    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²æ•°æ®"""
        history = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accs': self.train_accs,
            'val_accs': self.val_accs,
            'best_val_acc': self.best_val_acc
        }

        history_path = self.save_dir / 'training_history.json'
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=4)

        print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜: {history_path}")


def get_data_loaders(
        batch_size: int = 128,
        num_workers: int = 4,
        data_dir: str = "./data"
) -> Tuple[DataLoader, DataLoader]:
    """åˆ›å»ºMNISTæ•°æ®åŠ è½½å™¨

    Args:
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: æ•°æ®åŠ è½½çš„å·¥ä½œè¿›ç¨‹æ•°
        data_dir: æ•°æ®é›†ä¿å­˜ç›®å½•

    Returns:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
    """
    # 1. å‡å€¼å’Œæ ‡å‡†å·®å˜äº† (RGB 3é€šé“)
    cifar_mean = (0.4914, 0.4822, 0.4465)
    cifar_std = (0.2023, 0.1994, 0.2010)

    # 2. æ•°æ®å¢å¼ºä¸æ ‡å‡†åŒ–
    train_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4), # ç»å…¸çš„ CIFAR å¢å¼º
        transforms.RandomHorizontalFlip(),    # éšæœºæ°´å¹³ç¿»è½¬
        transforms.ToTensor(),
        transforms.Normalize(cifar_mean, cifar_std)
    ])

    val_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(cifar_mean, cifar_std)
    ])

    # 3. åŠ è½½æ•°æ®é›† (æ”¹ä¸º CIFAR10)
    train_dataset = datasets.CIFAR10( # åŸæ¥æ˜¯ FashionMNIST
        root=data_dir,
        train=True,
        download=True,
        transform=train_transform
    )

    val_dataset = datasets.CIFAR10( # åŸæ¥æ˜¯ FashionMNIST
        root=data_dir,
        train=False,
        download=True,
        transform=val_transform
    )


    # # æ•°æ®å¢å¼ºä¸æ ‡å‡†åŒ–
    # train_transform = transforms.Compose([
    #     transforms.RandomRotation(10),  # éšæœºæ—‹è½¬Â±10åº¦
    #     transforms.RandomAffine(0, translate=(0.1, 0.1)),  # éšæœºå¹³ç§»
    #     transforms.ToTensor(),
    #     transforms.Normalize((0.1307,), (0.3081,))  # MNISTçš„å‡å€¼å’Œæ ‡å‡†å·®
    # ])

    # val_transform = transforms.Compose([
    #     transforms.ToTensor(),
    #     transforms.Normalize((0.1307,), (0.3081,))
    # ])

    # # åŠ è½½æ•°æ®é›†
    # train_dataset = datasets.FashionMNIST(
    #     root=data_dir,
    #     train=True,
    #     download=True,
    #     transform=train_transform
    # )

    # val_dataset = datasets.FashionMNIST(
    #     root=data_dir,
    #     train=False,
    #     download=True,
    #     transform=val_transform
    # )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    return train_loader, val_loader


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒVision Mamba on MNIST')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--d_model', type=int, default=128,
                        help='éšè—å±‚ç»´åº¦ (é»˜è®¤: 128)')
    parser.add_argument('--n_layer', type=int, default=4,
                        help='Mambaå±‚æ•° (é»˜è®¤: 4)')
    parser.add_argument('--patch_size', type=int, default=4,
                        help='å›¾åƒå—å¤§å° (é»˜è®¤: 4)')
    parser.add_argument('--drop_rate', type=float, default=0.1,
                        help='Dropoutç‡ (é»˜è®¤: 0.1)')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=20,
                        help='è®­ç»ƒè½®æ•° (é»˜è®¤: 20)')
    parser.add_argument('--batch_size', type=int, default=128,
                        help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 128)')
    parser.add_argument('--lr', type=float, default=0.001,
                        help='åˆå§‹å­¦ä¹ ç‡ (é»˜è®¤: 0.001)')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                        help='æƒé‡è¡°å‡ (é»˜è®¤: 1e-4)')
    parser.add_argument('--num_workers', type=int, default=4,
                        help='æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: 4)')

    # è·¯å¾„å‚æ•°
    parser.add_argument('--data_dir', type=str, default='./data',
                        help='æ•°æ®é›†ç›®å½• (é»˜è®¤: ./data)')
    parser.add_argument('--save_dir', type=str, default='./checkpoints',
                        help='æ¨¡å‹ä¿å­˜ç›®å½• (é»˜è®¤: ./checkpoints)')
    parser.add_argument('--log_dir', type=str, default='./logs',
                        help='æ—¥å¿—ä¿å­˜ç›®å½• (é»˜è®¤: ./logs)')

    # å…¶ä»–å‚æ•°
    parser.add_argument('--resume', type=str, default=None,
                        help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„ (å¯é€‰)')
    parser.add_argument('--seed', type=int, default=42,
                        help='éšæœºç§å­ (é»˜è®¤: 42)')
    parser.add_argument('--device', type=str, default='cuda',
                        help='è®­ç»ƒè®¾å¤‡ (é»˜è®¤: cuda)')

    return parser.parse_args()


def set_seed(seed: int):
    """è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    # è§£æå‚æ•°
    args = parse_args()

    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)

    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“¦ åŠ è½½MNISTæ•°æ®é›†...")
    train_loader, val_loader = get_data_loaders(
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        data_dir=args.data_dir
    )

    # åˆ›å»ºæ¨¡å‹
    print("ğŸ”¨ åˆ›å»ºVision Mambaæ¨¡å‹...")
    model = create_vision_mamba_mnist(
        d_model=args.d_model,
        n_layer=args.n_layer,
        patch_size=args.patch_size,
        drop_rate=args.drop_rate
    )

    # å®šä¹‰æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()

    # å®šä¹‰ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay
    )

    # å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=args.epochs,
        eta_min=1e-6
    )

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        save_dir=args.save_dir,
        log_dir=args.log_dir
    )

    # å¼€å§‹è®­ç»ƒ
    trainer.train(num_epochs=args.epochs, resume=args.resume)


if __name__ == "__main__":
    main()